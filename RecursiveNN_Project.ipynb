{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9UJvB0skyAhb5TszSon+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghmaria-ch/fsopart4/blob/main/RecursiveNN_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvagP5C6kTIk",
        "outputId": "ae6d4498-f923-4ce9-ea37-d66c560ba48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "2Uwe_rMtkk6b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bufLwOdPkoQq",
        "outputId": "6cf8ba83-f27e-4e98-af30-c90d7a2d0d80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        documents.append((movie_reviews.raw(fileid), category))\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "print(\"Total samples:\", len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGsVXDs5kqZK",
        "outputId": "136211a1-2847-46f7-d27c-41e76520d2cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "\n",
        "for text, _ in documents:\n",
        "    words = word_tokenize(text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "all_words = sorted(set(all_words))\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(all_words)}\n",
        "\n",
        "print(\"Vocabulary size:\", len(word_to_ix))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfaK1o75kr-B",
        "outputId": "6af8cce3-fc6d-41bb-e39a-bba10875e54a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 46462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeNode:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.left = None\n",
        "        self.right = None\n"
      ],
      "metadata": {
        "id": "WuEzUpF_lP8T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(words):\n",
        "    if len(words) == 1:\n",
        "        index = word_to_ix.get(words[0], 0)\n",
        "        tensor = torch.zeros(1, len(word_to_ix))\n",
        "        tensor[0][index] = 1\n",
        "        return TreeNode(tensor)\n",
        "\n",
        "    mid = len(words) // 2\n",
        "\n",
        "    node = TreeNode(None)\n",
        "    node.left = build_tree(words[:mid])\n",
        "    node.right = build_tree(words[mid:])\n",
        "    return node\n"
      ],
      "metadata": {
        "id": "HfznGxKglTY0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecursiveNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size):\n",
        "        super(RecursiveNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Linear(vocab_size, hidden_size)\n",
        "        self.combine = nn.Linear(2 * hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, node):\n",
        "        if node.left is None and node.right is None:\n",
        "            return torch.tanh(self.embedding(node.value))\n",
        "\n",
        "        left_hidden = self.forward(node.left)\n",
        "        right_hidden = self.forward(node.right)\n",
        "\n",
        "        combined = torch.cat((left_hidden, right_hidden), dim=1)\n",
        "        return torch.tanh(self.combine(combined))\n"
      ],
      "metadata": {
        "id": "8Lzo39XelWFP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_ix)\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "\n",
        "model = RecursiveNN(vocab_size, hidden_size, output_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "nkAQ_wuqlXy2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "\n",
        "    for text, label in documents[:100]:\n",
        "        words = word_tokenize(text.lower())[:10]  # limit words\n",
        "\n",
        "        tree = build_tree(words)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden = model(tree)\n",
        "        output = model.classifier(hidden)\n",
        "\n",
        "        target = torch.tensor([0 if label == 'neg' else 1])\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqnNRokJlaUs",
        "outputId": "193d1bc2-9d0d-4869-884e-5aa6bbb0e38a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 70.1044\n",
            "Epoch 2, Loss: 38.3297\n",
            "Epoch 3, Loss: 2.0921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This movie was absolutely fantastic and amazing\"\n",
        "words = word_tokenize(sample_text.lower())\n",
        "\n",
        "tree = build_tree(words)\n",
        "hidden = model(tree)\n",
        "output = model.classifier(hidden)\n",
        "\n",
        "prediction = torch.argmax(output)\n",
        "\n",
        "print(\"Prediction:\", \"Positive\" if prediction.item() == 1 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhmQ_zmflzMW",
        "outputId": "3a804202-6b13-4bd8-958a-e170f1575524"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"This movie was absolutely terrible, boring, and a complete waste of time.\"\n",
        "words = word_tokenize(sample_text.lower())\n",
        "\n",
        "tree = build_tree(words)\n",
        "hidden = model(tree)\n",
        "output = model.classifier(hidden)\n",
        "\n",
        "prediction = torch.argmax(output)\n",
        "\n",
        "print(\"Prediction:\", \"Positive\" if prediction.item() == 1 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "livzpHSqmDnD",
        "outputId": "f8c1dadb-7a8c-4c2f-c085-e14833af77fb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Negative\n"
          ]
        }
      ]
    }
  ]
}